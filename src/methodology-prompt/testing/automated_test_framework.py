#!/usr/bin/env python3
"""
BOS Methodology Prompt - Automated Test Framework

Comprehensive automated testing system for validating BOS methodology prompt behavior,
quality scoring, and methodology compliance across different scenarios and personas.
"""

import json
import re
import time
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import pytest
import unittest


class PersonaType(Enum):
    PRODUCT_OWNER = "product_owner"
    DEVELOPER = "developer"
    PLATFORM_SRE = "platform_sre"


class ValidationResult(Enum):
    PASS = "pass"
    FAIL = "fail"
    WARNING = "warning"


@dataclass
class TestScenario:
    """Test scenario definition for automated testing."""
    name: str
    business_process: str
    persona: PersonaType
    test_inputs: List[str]
    expected_outputs: Dict[str, Any]
    validation_criteria: Dict[str, Any]


@dataclass
class ValidationRule:
    """Validation rule for automated quality assessment."""
    rule_name: str
    rule_type: str  # completeness, quality, consistency, methodology
    check_function: str
    threshold: float
    weight: float
    persona_specific: Optional[PersonaType] = None


@dataclass
class TestResult:
    """Test execution result with detailed metrics."""
    scenario_name: str
    persona: PersonaType
    execution_time: float
    validation_score: float
    quality_score: float
    completeness_score: float
    consistency_score: float
    methodology_compliance: float
    errors: List[str]
    warnings: List[str]
    detailed_results: Dict[str, Any]


class BOSPromptValidator:
    """Core validation engine for BOS methodology prompt responses."""
    
    def __init__(self):
        self.validation_rules = self._initialize_validation_rules()
        self.stakeholder_patterns = self._initialize_stakeholder_patterns()
        self.methodology_requirements = self._initialize_methodology_requirements()
    
    def _initialize_validation_rules(self) -> List[ValidationRule]:
        """Initialize comprehensive validation rule set."""
        return [
            # Completeness Rules
            ValidationRule("stakeholder_categories", "completeness", "check_stakeholder_categories", 0.8, 0.25),
            ValidationRule("impact_categories", "completeness", "check_impact_categories", 0.8, 0.25),
            ValidationRule("telemetry_coverage", "completeness", "check_telemetry_coverage", 0.7, 0.2),
            ValidationRule("signal_definitions", "completeness", "check_signal_definitions", 0.7, 0.3),
            
            # Quality Rules
            ValidationRule("measurable_expectations", "quality", "check_measurable_expectations", 0.8, 0.3),
            ValidationRule("specific_timeframes", "quality", "check_specific_timeframes", 0.6, 0.2),
            ValidationRule("quantified_impacts", "quality", "check_quantified_impacts", 0.7, 0.3),
            ValidationRule("technical_precision", "quality", "check_technical_precision", 0.7, 0.2),
            
            # Consistency Rules
            ValidationRule("stakeholder_consistency", "consistency", "check_stakeholder_consistency", 0.9, 0.4),
            ValidationRule("impact_traceability", "consistency", "check_impact_traceability", 0.8, 0.3),
            ValidationRule("signal_alignment", "consistency", "check_signal_alignment", 0.8, 0.3),
            
            # Methodology Compliance Rules
            ValidationRule("bos_framework_adherence", "methodology", "check_bos_framework", 1.0, 0.4),
            ValidationRule("step_progression", "methodology", "check_step_progression", 1.0, 0.3),
            ValidationRule("persona_appropriateness", "methodology", "check_persona_guidance", 0.8, 0.3),
        ]
    
    def _initialize_stakeholder_patterns(self) -> Dict[str, List[str]]:
        """Initialize patterns for stakeholder validation."""
        return {
            "people": [
                r"(manager|officer|analyst|specialist|representative|coordinator)",
                r"(team|department|group).*?(lead|head|director)",
                r"(customer service|support|operations|sales|marketing)"
            ],
            "business_entities": [
                r"(department|division|business unit|subsidiary)",
                r"(customer|client|partner|vendor)",
                r"(regulatory|compliance|audit|legal)"
            ],
            "vendors": [
                r"(provider|supplier|vendor|contractor)",
                r"(service|platform|system|tool).*?(provider|vendor)",
                r"(third.?party|external|outsourced)"
            ]
        }
    
    def _initialize_methodology_requirements(self) -> Dict[str, Dict]:
        """Initialize BOS methodology validation requirements."""
        return {
            "step_1_stakeholders": {
                "required_categories": ["people", "business_entities", "vendors"],
                "minimum_per_category": 1,
                "required_fields": ["role", "expectations", "criticality"]
            },
            "step_2_dependencies": {
                "required_mapping": ["stakeholder_to_process", "process_to_stakeholder"],
                "measurability_required": True,
                "criticality_assessment": True
            },
            "step_3_impacts": {
                "required_categories": ["financial", "legal", "operational", "customer_experience"],
                "quantification_required": True,
                "recovery_objectives": True
            },
            "step_4_telemetry": {
                "observable_units": True,
                "telemetry_sources": True,
                "gaps_identified": True
            },
            "step_5_signals": {
                "business_signals": True,
                "process_signals": True,
                "system_signals": True,
                "thresholds_defined": True
            }
        }
    
    def validate_prompt_response(self, response: str, scenario: TestScenario) -> TestResult:
        """Execute comprehensive validation of prompt response."""
        start_time = time.time()
        
        # Parse response structure
        parsed_response = self._parse_response(response)
        
        # Execute validation rules
        validation_results = {}
        errors = []
        warnings = []
        
        for rule in self.validation_rules:
            if rule.persona_specific and rule.persona_specific != scenario.persona:
                continue
                
            try:
                result = self._execute_validation_rule(rule, parsed_response, scenario)
                validation_results[rule.rule_name] = result
                
                if result["score"] < rule.threshold:
                    if rule.weight > 0.3:  # High weight rules are errors
                        errors.append(f"{rule.rule_name}: {result['message']}")
                    else:
                        warnings.append(f"{rule.rule_name}: {result['message']}")
                        
            except Exception as e:
                errors.append(f"Validation error in {rule.rule_name}: {str(e)}")
        
        # Calculate composite scores
        scores = self._calculate_scores(validation_results)
        
        execution_time = time.time() - start_time
        
        return TestResult(
            scenario_name=scenario.name,
            persona=scenario.persona,
            execution_time=execution_time,
            validation_score=scores["validation"],
            quality_score=scores["quality"],
            completeness_score=scores["completeness"],
            consistency_score=scores["consistency"],
            methodology_compliance=scores["methodology"],
            errors=errors,
            warnings=warnings,
            detailed_results=validation_results
        )
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse prompt response into structured data."""
        parsed = {
            "commands_detected": [],
            "stakeholder_mentions": [],
            "impact_categories": [],
            "telemetry_references": [],
            "signal_definitions": [],
            "measurable_metrics": [],
            "time_references": [],
            "technical_terms": [],
            "business_language": [],
            "validation_prompts": []
        }
        
        # Extract commands
        command_pattern = r'/(\w+)(?:\s+([^/\n]+))?'
        parsed["commands_detected"] = re.findall(command_pattern, response)
        
        # Extract stakeholder mentions
        for category, patterns in self.stakeholder_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, response, re.IGNORECASE)
                parsed["stakeholder_mentions"].extend([(category, match) for match in matches])
        
        # Extract measurable metrics
        metric_patterns = [
            r'(\d+(?:\.\d+)?)\s*%',  # Percentages
            r'(\d+(?:\.\d+)?)\s*(seconds?|minutes?|hours?|days?)',  # Time units
            r'\$(\d+(?:,\d{3})*(?:\.\d{2})?)',  # Dollar amounts
            r'(\d+(?:\.\d+)?)\s*(ms|sec|min|hr)',  # Technical time units
        ]
        
        for pattern in metric_patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            parsed["measurable_metrics"].extend(matches)
        
        # Extract impact categories
        impact_keywords = {
            "financial": ["revenue", "cost", "profit", "loss", "budget", "financial"],
            "legal": ["compliance", "regulatory", "legal", "audit", "violation"],
            "operational": ["process", "workflow", "operation", "efficiency", "productivity"],
            "customer_experience": ["customer", "user", "satisfaction", "experience", "service"]
        }
        
        for category, keywords in impact_keywords.items():
            for keyword in keywords:
                if re.search(rf'\b{keyword}\b', response, re.IGNORECASE):
                    parsed["impact_categories"].append(category)
        
        return parsed
    
    def _execute_validation_rule(self, rule: ValidationRule, parsed_response: Dict, scenario: TestScenario) -> Dict:
        """Execute a specific validation rule."""
        method_name = rule.check_function
        if hasattr(self, method_name):
            check_method = getattr(self, method_name)
            return check_method(parsed_response, scenario)
        else:
            return {"score": 0.0, "message": f"Validation method {method_name} not implemented"}
    
    def check_stakeholder_categories(self, parsed_response: Dict, scenario: TestScenario) -> Dict:
        """Check if all stakeholder categories are addressed."""
        mentioned_categories = set([category for category, _ in parsed_response["stakeholder_mentions"]])
        required_categories = set(["people", "business_entities", "vendors"])
        
        coverage = len(mentioned_categories.intersection(required_categories)) / len(required_categories)
        
        return {
            "score": coverage,
            "message": f"Stakeholder category coverage: {coverage:.1%}",
            "details": {
                "mentioned": list(mentioned_categories),
                "required": list(required_categories),
                "missing": list(required_categories - mentioned_categories)
            }
        }
    
    def check_impact_categories(self, parsed_response: Dict, scenario: TestScenario) -> Dict:
        """Check if all impact categories are addressed."""
        mentioned_impacts = set(parsed_response["impact_categories"])
        required_impacts = set(["financial", "legal", "operational", "customer_experience"])
        
        coverage = len(mentioned_impacts.intersection(required_impacts)) / len(required_impacts)
        
        return {
            "score": coverage,
            "message": f"Impact category coverage: {coverage:.1%}",
            "details": {
                "mentioned": list(mentioned_impacts),
                "required": list(required_impacts),
                "missing": list(required_impacts - mentioned_impacts)
            }
        }
    
    def check_measurable_expectations(self, parsed_response: Dict, scenario: TestScenario) -> Dict:
        """Check for measurable, quantified expectations."""
        metrics_count = len(parsed_response["measurable_metrics"])
        stakeholder_count = len(set([mention[1] for mention in parsed_response["stakeholder_mentions"]]))
        
        if stakeholder_count == 0:
            ratio = 0.0
        else:
            ratio = min(metrics_count / stakeholder_count, 1.0)
        
        return {
            "score": ratio,
            "message": f"Measurable expectations ratio: {ratio:.1%}",
            "details": {
                "metrics_found": metrics_count,
                "stakeholders_mentioned": stakeholder_count,
                "metrics": parsed_response["measurable_metrics"]
            }
        }
    
    def check_bos_framework(self, parsed_response: Dict, scenario: TestScenario) -> Dict:
        """Check adherence to BOS methodology framework."""
        # Check for proper BOS step progression prompts
        framework_indicators = [
            "WHO depends", "WHAT they expect", "WHAT breaks", 
            "WHAT telemetry", "WHAT signals", "PLAYBOOK", "DASHBOARD"
        ]
        
        framework_mentions = 0
        for indicator in framework_indicators:
            if indicator.lower() in parsed_response.get("raw_text", "").lower():
                framework_mentions += 1
        
        score = framework_mentions / len(framework_indicators)
        
        return {
            "score": score,
            "message": f"BOS framework adherence: {score:.1%}",
            "details": {
                "framework_mentions": framework_mentions,
                "total_indicators": len(framework_indicators)
            }
        }
    
    def _calculate_scores(self, validation_results: Dict) -> Dict[str, float]:
        """Calculate composite scores from validation results."""
        scores = {
            "validation": 0.0,
            "quality": 0.0,
            "completeness": 0.0,
            "consistency": 0.0,
            "methodology": 0.0
        }
        
        # Calculate weighted scores by category
        category_weights = {}
        category_scores = {}
        
        for rule in self.validation_rules:
            if rule.rule_name in validation_results:
                result = validation_results[rule.rule_name]
                category = rule.rule_type
                
                if category not in category_weights:
                    category_weights[category] = 0.0
                    category_scores[category] = 0.0
                
                category_weights[category] += rule.weight
                category_scores[category] += result["score"] * rule.weight
        
        # Normalize scores
        for category in category_scores:
            if category_weights[category] > 0:
                scores[category] = category_scores[category] / category_weights[category]
        
        # Overall validation score
        scores["validation"] = sum(scores[cat] for cat in ["quality", "completeness", "consistency", "methodology"]) / 4
        
        return scores


class BOSPromptTestSuite:
    """Comprehensive test suite for BOS methodology prompt system."""
    
    def __init__(self):
        self.validator = BOSPromptValidator()
        self.test_scenarios = self._initialize_test_scenarios()
    
    def _initialize_test_scenarios(self) -> List[TestScenario]:
        """Initialize comprehensive test scenario library."""
        return [
            # Product Owner Scenarios
            TestScenario(
                name="po_loan_approval_stakeholders",
                business_process="loan_approval_process",
                persona=PersonaType.PRODUCT_OWNER,
                test_inputs=[
                    "/start loan_approval_process",
                    "/persona product_owner",
                    "Loan officers are the primary stakeholders"
                ],
                expected_outputs={
                    "stakeholder_categories": ["people"],
                    "business_focus": True,
                    "measurable_expectations": True
                },
                validation_criteria={
                    "stakeholder_completeness": 0.8,
                    "business_language": 0.9,
                    "measurability": 0.7
                }
            ),
            
            # Developer Scenarios
            TestScenario(
                name="dev_payment_processing_telemetry",
                business_process="payment_processing_service",
                persona=PersonaType.DEVELOPER,
                test_inputs=[
                    "/start payment_processing_service",
                    "/persona developer",
                    "Payment confirmation API is the critical observable unit"
                ],
                expected_outputs={
                    "technical_focus": True,
                    "observable_units": True,
                    "telemetry_mapping": True
                },
                validation_criteria={
                    "technical_precision": 0.8,
                    "observable_unit_coverage": 0.7,
                    "implementation_feasibility": 0.8
                }
            ),
            
            # Platform SRE Scenarios
            TestScenario(
                name="sre_auth_system_signals",
                business_process="user_authentication_system",
                persona=PersonaType.PLATFORM_SRE,
                test_inputs=[
                    "/start user_authentication_system",
                    "/persona platform_sre",
                    "Authentication system availability signals are critical"
                ],
                expected_outputs={
                    "infrastructure_focus": True,
                    "system_signals": True,
                    "operational_monitoring": True
                },
                validation_criteria={
                    "infrastructure_coverage": 0.8,
                    "system_signal_quality": 0.8,
                    "operational_feasibility": 0.9
                }
            )
        ]
    
    async def run_comprehensive_test_suite(self) -> Dict[str, Any]:
        """Execute comprehensive automated test suite."""
        test_results = []
        suite_start_time = time.time()
        
        for scenario in self.test_scenarios:
            # Simulate prompt execution (in real implementation, this would call actual LLM)
            simulated_response = self._simulate_prompt_response(scenario)
            
            # Validate response
            result = self.validator.validate_prompt_response(simulated_response, scenario)
            test_results.append(result)
        
        suite_execution_time = time.time() - suite_start_time
        
        # Generate comprehensive report
        report = self._generate_test_report(test_results, suite_execution_time)
        
        return report
    
    def _simulate_prompt_response(self, scenario: TestScenario) -> str:
        """Simulate prompt response for testing (replace with actual LLM call in production)."""
        # This is a placeholder - in real implementation, would execute actual prompt
        templates = {
            PersonaType.PRODUCT_OWNER: """
âœ… Persona Set: Product Owner

Primary Stakeholder Identified: {stakeholder_type}

Business context focus with measurable expectations:
- Response time: 24 hours
- Accuracy target: 95%
- Financial impact: $50,000 revenue risk

Impact categories to analyze:
- Financial: Revenue loss potential
- Operational: Process efficiency impact
- Customer Experience: Satisfaction degradation
- Legal: Compliance requirements

Next: Complete stakeholder mapping across People/Business Entities/Vendors categories.
            """,
            PersonaType.DEVELOPER: """
âœ… Technical Stakeholder Profile Complete

Observable Units Identified:
- Payment confirmation API endpoint
- Database transaction handler
- External payment processor integration

Current telemetry sources:
- API response time metrics (500ms SLA)
- Error rate monitoring (99.9% availability target)
- Transaction success rate tracking

Process signals required:
- Payment validation success/failure
- Response time thresholds
- Error pattern detection

Technical implementation approach validated.
            """,
            PersonaType.PLATFORM_SRE: """
âœ… Infrastructure Dependencies Mapped

System Signal Requirements:
- Authentication service availability (99.9% uptime)
- Database connection health monitoring
- Load balancer performance metrics

Infrastructure integration points:
- APM monitoring system connection
- Dashboard widget specifications
- Alert threshold configurations

Operational monitoring approach:
- System health indicators
- Performance degradation detection
- Automated recovery procedures

Dashboard requirements technically feasible.
            """
        }
        
        return templates.get(scenario.persona, "Generic response for testing")
    
    def _generate_test_report(self, test_results: List[TestResult], execution_time: float) -> Dict[str, Any]:
        """Generate comprehensive automated test report."""
        total_tests = len(test_results)
        passed_tests = len([r for r in test_results if r.validation_score >= 0.8])
        
        average_scores = {
            "validation": sum(r.validation_score for r in test_results) / total_tests,
            "quality": sum(r.quality_score for r in test_results) / total_tests,
            "completeness": sum(r.completeness_score for r in test_results) / total_tests,
            "consistency": sum(r.consistency_score for r in test_results) / total_tests,
            "methodology": sum(r.methodology_compliance for r in test_results) / total_tests,
        }
        
        all_errors = []
        all_warnings = []
        for result in test_results:
            all_errors.extend(result.errors)
            all_warnings.extend(result.warnings)
        
        return {
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "pass_rate": passed_tests / total_tests,
                "execution_time": execution_time,
                "average_scores": average_scores
            },
            "detailed_results": [asdict(result) for result in test_results],
            "quality_analysis": {
                "critical_errors": len(all_errors),
                "warnings": len(all_warnings),
                "error_patterns": self._analyze_error_patterns(all_errors),
                "improvement_recommendations": self._generate_recommendations(test_results)
            },
            "persona_performance": {
                persona.value: {
                    "avg_validation_score": sum(r.validation_score for r in test_results if r.persona == persona) / len([r for r in test_results if r.persona == persona]),
                    "test_count": len([r for r in test_results if r.persona == persona])
                }
                for persona in PersonaType
            }
        }
    
    def _analyze_error_patterns(self, errors: List[str]) -> Dict[str, int]:
        """Analyze common error patterns for improvement insights."""
        patterns = {}
        for error in errors:
            error_type = error.split(":")[0] if ":" in error else "unknown"
            patterns[error_type] = patterns.get(error_type, 0) + 1
        return patterns
    
    def _generate_recommendations(self, test_results: List[TestResult]) -> List[str]:
        """Generate improvement recommendations based on test results."""
        recommendations = []
        
        # Analyze score patterns
        avg_completeness = sum(r.completeness_score for r in test_results) / len(test_results)
        avg_quality = sum(r.quality_score for r in test_results) / len(test_results)
        avg_consistency = sum(r.consistency_score for r in test_results) / len(test_results)
        
        if avg_completeness < 0.8:
            recommendations.append("Improve stakeholder and impact category coverage")
        
        if avg_quality < 0.8:
            recommendations.append("Enhance measurability and specificity of expectations")
        
        if avg_consistency < 0.8:
            recommendations.append("Strengthen cross-step data consistency validation")
        
        return recommendations


# Test execution functions
async def run_automated_tests():
    """Main function to execute automated test suite."""
    test_suite = BOSPromptTestSuite()
    
    print("ðŸ¤– Starting BOS Methodology Prompt Automated Test Suite...")
    
    results = await test_suite.run_comprehensive_test_suite()
    
    print(f"\nðŸ“Š Test Suite Completed in {results['summary']['execution_time']:.2f} seconds")
    print(f"âœ… Pass Rate: {results['summary']['pass_rate']:.1%}")
    print(f"ðŸ“ˆ Average Validation Score: {results['summary']['average_scores']['validation']:.1%}")
    
    return results


if __name__ == "__main__":
    # Run automated tests
    results = asyncio.run(run_automated_tests())
    
    # Save results to file
    with open("automated_test_results.json", "w") as f:
        json.dump(results, f, indent=2, default=str)
    
    print("\nðŸ“ Results saved to automated_test_results.json")